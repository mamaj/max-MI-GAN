{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.kl import kl_divergence as kl\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from storch.method import ScoreFunction\n",
    "import storch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from model import MaxIMGAN\n",
    "from utils import display, load_mnist, create_digit_grid, get_logdir\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "latent_dim = 128\n",
    "n_mixtures = 1\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.0002\n",
    "adam_betas=(0.5, 0.999)\n",
    "beta = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "x_size = 20\n",
    "\n",
    "mnist, _ = load_mnist()\n",
    "mnist_lowres, _ = load_mnist(size=x_size)\n",
    "\n",
    "y_loader = DataLoader(mnist, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "x_loader = DataLoader(mnist_lowres, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "grid = create_digit_grid(mnist_lowres, cols=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = MaxIMGAN(input_shape=(1, x_size, x_size),\n",
    "                 output_shape=(1, 28, 28),\n",
    "                 n_mixtures=n_mixtures)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "gen_optim = torch.optim.Adam(lr=lr, params=model.generator.parameters(), betas=adam_betas)\n",
    "disc_optim = torch.optim.Adam(lr=lr, params=model.discriminator.parameters(), betas=adam_betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94897fb0b074193ac1f6d647334c413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafdfcba37f34d7e966b80b0415b6e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0d6d043fde444589933dd5999bc729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ef66f4c50e469d8799ab9ef9e76a8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=33'>34</a>\u001b[0m gen_loss \u001b[39m=\u001b[39m (\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=34'>35</a>\u001b[0m     F\u001b[39m.\u001b[39mbinary_cross_entropy_with_logits(disc_logit, labels) \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=35'>36</a>\u001b[0m     \u001b[39m# - beta * y_fake_dist.log_prob(y_fake).mean()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=36'>37</a>\u001b[0m     \u001b[39m+\u001b[39m beta \u001b[39m*\u001b[39m y_fake_dist\u001b[39m.\u001b[39mentropy()\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=37'>38</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=39'>40</a>\u001b[0m gen_optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=40'>41</a>\u001b[0m gen_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=41'>42</a>\u001b[0m gen_optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/reza/Projects/max-MI-GAN/maxIMGAN.ipynb#ch0000008?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m it \u001b[39m%\u001b[39m \u001b[39m25\u001b[39m \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(get_logdir())\n",
    "it = 0\n",
    "\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    for (x, _), (y_real, _) in tqdm(zip(x_loader, y_loader), total=len(x_loader)):\n",
    "        y_real, x = y_real.to(DEVICE), x.to(DEVICE)\n",
    "        \n",
    "        # train the discriminator\n",
    "        y_fake_dist = model(x)\n",
    "        y_fake = y_fake_dist.sample()\n",
    "        \n",
    "        y = torch.cat((y_real, y_fake), dim=0)\n",
    "        labels = torch.cat((\n",
    "            torch.ones((batch_size, 1)),\n",
    "            torch.zeros((batch_size, 1))\n",
    "            )).to(DEVICE)\n",
    "        \n",
    "        disc_logit = model.discriminator(y)\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(disc_logit, labels)\n",
    "    \n",
    "        disc_optim.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        disc_optim.step()\n",
    "        \n",
    "        \n",
    "        # train the generator\n",
    "        # y_fake_dist = model(x)\n",
    "        y_fake = y_fake_dist.rsample()\n",
    "        \n",
    "        \n",
    "        disc_logit = model.discriminator(y_fake)\n",
    "        labels = torch.ones(disc_logit.shape).to(DEVICE)\n",
    "        gen_loss = (\n",
    "            F.binary_cross_entropy_with_logits(disc_logit, labels) \n",
    "            # - beta * y_fake_dist.log_prob(y_fake).mean()\n",
    "            + beta * y_fake_dist.entropy().mean()\n",
    "            )\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        gen_optim.step()\n",
    "        \n",
    "        \n",
    "        if it % 25 == 1:\n",
    "            with torch.no_grad():\n",
    "                y = model(grid).sample()\n",
    "            writer.add_scalar('loss/gen', gen_loss, it)\n",
    "            writer.add_scalar('loss/disc', disc_loss, it)\n",
    "            writer.add_images('images/x', grid, it)\n",
    "            writer.add_images('images/y', y, it)\n",
    "\n",
    "        it += 1\n",
    "                \n",
    "    if epoch % 25 == 0 and epoch != 0:\n",
    "        torch.save(model, f'checkpoints/model_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46efcf6d1b94e679aa47df412493c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d1be45b4be49ca845dedc773efe3f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/937 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_func = ScoreFunction(\"e\", n_samples=200, baseline_factory='moving_average')\n",
    "\n",
    "writer = SummaryWriter('runs_maximgan')\n",
    "it = 0\n",
    "\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    for (x, _), (y_real, _) in tqdm(zip(x_loader, y_loader), total=len(x_loader)):\n",
    "        y_real, x = y_real.to(DEVICE), x.to(DEVICE)\n",
    "        \n",
    "        # train the discriminator\n",
    "        y_fake = model(x).sample()\n",
    "        \n",
    "        y = torch.cat((y_real, y_fake), dim=0)\n",
    "        labels = torch.cat((\n",
    "            torch.ones((batch_size, 1)),\n",
    "            torch.zeros((batch_size, 1))\n",
    "            )).to(DEVICE)\n",
    "        \n",
    "        disc_logit = model.discriminator(y)\n",
    "        disc_loss = F.binary_cross_entropy_with_logits(disc_logit, labels)\n",
    "    \n",
    "        disc_optim.zero_grad()\n",
    "        disc_loss.backward()\n",
    "        disc_optim.step()\n",
    "        \n",
    "        \n",
    "        # train the generator\n",
    "        y_fake_dist = model(x)\n",
    "        y_fake = score_func(y_fake_dist)\n",
    "        # y_fake = y_fake_dist.rsample()\n",
    "        \n",
    "        \n",
    "        disc_logit = model.discriminator(y_fake)\n",
    "        label = torch.ones(disc_logit.shape).to(DEVICE)\n",
    "        gen_loss = (\n",
    "            F.binary_cross_entropy_with_logits(disc_logit, label, reduction='none')[:, :, 0] \n",
    "            # - beta * y_fake_dist.log_prob(y_fake).mean()\n",
    "            # + beta * y_fake_dist.entropy().mean()\n",
    "            - beta * y_fake_dist.log_prob(y_fake)\n",
    "            ).mean(-1)\n",
    "        \n",
    "        gen_optim.zero_grad()\n",
    "        storch.add_cost(gen_loss, \"gen_loss\")\n",
    "        storch.backward()\n",
    "        gen_optim.step()\n",
    "        \n",
    "        \n",
    "        if it % 25 == 1:\n",
    "            pass\n",
    "            # writer.add_scalar('loss/gen', gen_loss, it)\n",
    "            # writer.add_scalar('loss/disc', disc_loss, it)\n",
    "        \n",
    "        it += 1\n",
    "        \n",
    "        break\n",
    "    break\n",
    "        \n",
    "    if epoch % 25 == 0 and epoch != 0:\n",
    "        torch.save(model, f'checkpoints/model_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 64, 1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_fake.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display samples\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "display(model(x).sample(), axs[0])\n",
    "display(x, axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.1931)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = td.AffineTransform(-1, 2)\n",
    "p = td.TransformedDistribution(td.Beta(1, 2), f)\n",
    "\n",
    "p.base_dist.entropy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "553daaaae048e5495e5ec465004ab469f58a240294fad91e19ceb6f6096d177c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
